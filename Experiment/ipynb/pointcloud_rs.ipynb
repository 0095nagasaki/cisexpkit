{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pointcloud_rs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYQZsJ7316zA"
      },
      "source": [
        "# 逆透視変換による3次元復元"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GJGhGV516zJ"
      },
      "source": [
        "## RealSenseを接続してください．\n",
        "pyrealsenseを使います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFSkEVwutRBD"
      },
      "source": [
        "import pyrealsense2 as rs\n",
        "import numpy as np\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Configure color and depth to run at VGA resolution at 30 frames per second\n",
        "config = rs.config()\n",
        "config.enable_stream(rs.stream.depth)\n",
        "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
        "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_GULHX16zL"
      },
      "source": [
        "## カラー画像と深度画像を取得して表示・保存します．\n",
        "スペースキーを押す毎に画像が連番で保存されます．'q'を押すと終了します．最後に保存したデータが点群の作成に使われます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2qNjk_GtRBS"
      },
      "source": [
        "# Start streaming\n",
        "pipeline = rs.pipeline()\n",
        "profile = pipeline.start(config)\n",
        "\n",
        "# Get camera parameters\n",
        "intr = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
        "scale = config.resolve(rs.pipeline_wrapper(pipeline)).get_device().first_depth_sensor().get_depth_scale()\n",
        "\n",
        "print(\"focal length(x) in pixels = \", intr.fx)\n",
        "print(\"focal length(y) in pixels = \", intr.fy)\n",
        "print(\"image height = \", intr.height)\n",
        "print(\"image width = \", intr.width)\n",
        "print(\"ppx = \", intr.ppx)\n",
        "print(\"ppy = \", intr.ppy)\n",
        "\n",
        "# Create a camera alignment object (depth aligned to color)\n",
        "align = rs.align(rs.stream.color)\n",
        "max_depth = 2.0 / scale # Zeros out for any depth greater than 2.0 meters\n",
        "\n",
        "# Display and save images\n",
        "print(\"Press [SPACE] to save images (png) and depth data (npy).\")\n",
        "print(\"Press 'q' to stop.\")\n",
        "nsaved = 0\n",
        "try:\n",
        "    while True:\n",
        "        # Wait for a coherent pair of frames: depth and color\n",
        "        frames = pipeline.wait_for_frames()\n",
        "        aligned_frames = align.process(frames)\n",
        "        color_frame = aligned_frames.get_color_frame()\n",
        "        depth_frame = aligned_frames.get_depth_frame()\n",
        "        if not depth_frame or not color_frame:\n",
        "            continue\n",
        "\n",
        "        # Convert images to numpy arrays\n",
        "        bgr = np.asanyarray(color_frame.get_data())\n",
        "        depth = np.asanyarray(depth_frame.get_data())\n",
        "        depth[depth > max_depth] = 0 # Zeros out\n",
        "\n",
        "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
        "        depth_colormap = cv2.applyColorMap(cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U), \n",
        "                                           cv2.COLORMAP_JET)\n",
        "  \n",
        "        images = np.hstack((bgr, depth_colormap))\n",
        "        # Show images\n",
        "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
        "        cv2.imshow('RealSense', images)\n",
        "        \n",
        "        key = cv2.waitKey(33)\n",
        "        if key == ord(' '):\n",
        "            Z = depth * scale * 1e+3 # unit in mm\n",
        "            color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Save images\n",
        "            cv2.imwrite('color{:02d}pc.png'.format(nsaved), bgr)\n",
        "            cv2.imwrite('depth{:02d}pc.png'.format(nsaved), depth_colormap)\n",
        "            np.save('Z{:02d}pc.npy'.format(nsaved), Z)\n",
        "            \n",
        "            print(\"color image and depth data are saved ({:02d})\".format(nsaved))\n",
        "            nsaved += 1\n",
        "\n",
        "        elif key == ord('q'):\n",
        "            if nsaved == 0:\n",
        "                Z = depth * scale * 1e+3 # unit in mm\n",
        "                color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "            cv2.destroyAllWindows()\n",
        "            break\n",
        "        \n",
        "finally:\n",
        "    # Stop streaming\n",
        "    pipeline.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n06_TE8y16zO"
      },
      "source": [
        "## 逆透視変換でポイントクラウドを作りましょう．\n",
        "カラー画像および点群を作成する深度データを可視化します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FmUdTtztRBW"
      },
      "source": [
        "from PIL import Image\n",
        "height, width, _ = color.shape\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(color)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(Z, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ibU4M616zQ"
      },
      "source": [
        "### 各画素にuとvの座標を設定します．座標系は[予習事項](https://github.com/tsakailab/cisexpkit/blob/master/Experiment/Document/preparation.pdf)の図1です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVFSGtoEtRBX"
      },
      "source": [
        "# 主点を設定します．\n",
        "cx, cy = intr.ppx, intr.ppy # width*0.5, height*0.5\n",
        "j_to_u = lambda j: -(j - cx)\n",
        "i_to_v = lambda i: -(i - cy)\n",
        "# 画像平面の座標を設定します．\n",
        "u, v = np.meshgrid(j_to_u(np.arange(width)), i_to_v(np.arange(height)))\n",
        "print(u, u.shape)\n",
        "print(v, v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL3StI9816zQ"
      },
      "source": [
        "### Z, u, vが与えられたとき，X[mm]とY[mm]を計算する関数を作りましょう．[ヒント：予習事項の問2](https://github.com/tsakailab/cisexpkit/blob/master/Experiment/Document/preparation.pdf)\n",
        "\n",
        "Q11: 関数 Zuv_to_XYを完成させよ．単位mmの深度Zと，画素数の単位をもつ座標u, v, 焦点距離fが与えられているものとする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMnVo0cOtRBZ"
      },
      "source": [
        "focal_length = [intr.fx, intr.fy] # in pixels\n",
        "def Zuv_to_XY(Z, u, v, f=focal_length):\n",
        "    ### X = ________   # Z, u, v, f[0], f[1] から必要なものを使って計算する\n",
        "    ### Y = ________   # Z, u, v, f[0], f[1] から必要なものを使って計算する\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OWR-JBMtRBa",
        "scrolled": true
      },
      "source": [
        "# Z, u, v から X, Y を計算します．\n",
        "X, Y = Zuv_to_XY(Z, u, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwbq-v1z16zU"
      },
      "source": [
        "## ポイントクラウドを可視化して観察しましょう．\n",
        "\n",
        "Q12: 使用した画像のサイズと全画素数，および深度画像の画素値がゼロでない画素の数はいくらか．\n",
        "\n",
        "Q13: 逆透視変換できない点が生じる原因を複数述べよ．\n",
        "\n",
        "Q14: [plotly](https://plotly.com/)とは何か．特に，[plotly Graphing Libraries](https://plotly.com/graphing-libraries/)はどのような特長があるか．\n",
        "\n",
        "Q15: [plotly.graph_objects.Scatter3d](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter3d.html#plotly.graph_objects.Scatter3d)は何ができる関数で，このサンプルコードではどのように使われているか．\n",
        "> マウスで視点や拡大・縮小をコントロールできます．描画領域右上にある機能も活用しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_nPc9jtRBb",
        "scrolled": false
      },
      "source": [
        "nd = np.count_nonzero(Z)\n",
        "n = 30000\n",
        "p = np.random.choice(nd, min(n,nd), replace=False)\n",
        "print(\"%d out of %d points are displayed.\" % (n, nd))\n",
        "\n",
        "import plotly.graph_objs  as go\n",
        "rgb = color[Z>0][p] # * 1.5 # brighter\n",
        "\n",
        "trace = go.Scatter3d(x=X[Z>0][p], y=Y[Z>0][p], z=Z[Z>0][p], mode='markers',\n",
        "                     marker=dict(size=2, \n",
        "                                color=['rgb({},{},{})'.format(r,g,b) for r,g,b in zip(rgb[:,0], rgb[:,1], rgb[:,2])],\n",
        "                                opacity=0.8))\n",
        "\n",
        "layout = go.Layout(margin=dict(l=0,r=0,b=0,t=0))\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "camera = dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=-0.4, z=0), eye=dict(x=0, y=0.8, z=-2))\n",
        "fig.update_layout(scene_camera=camera)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}