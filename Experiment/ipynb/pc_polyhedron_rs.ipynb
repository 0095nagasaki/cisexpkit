{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pc_polyhedron_rs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqde_xlr2P4C"
      },
      "source": [
        "# ポイントクラウドを用いた多面体の計測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90PwT0112P4Q"
      },
      "source": [
        "## RealSenseで多面体を撮像してください．\n",
        "> なるべく多面体の2つ以上の面が広く写るように撮影しましょう．\n",
        "\n",
        "> このファイルの内容は，★印まで pointcloud_rs.ipynb と同じです．★印まで同様に作業を進めてください． "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFSkEVwutRBD"
      },
      "source": [
        "import pyrealsense2 as rs\n",
        "import numpy as np\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Configure color and depth to run at VGA resolution at 30 frames per second\n",
        "config = rs.config()\n",
        "config.enable_stream(rs.stream.depth)\n",
        "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
        "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mUSNHI2P4X"
      },
      "source": [
        "## カラー画像と深度画像を取得して表示・保存します．\n",
        "スペースキーを押す毎に画像が連番で保存されます．'q'を押すと終了します．最後に保存したデータが点群の作成に使われます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2qNjk_GtRBS"
      },
      "source": [
        "# Start streaming\n",
        "pipeline = rs.pipeline()\n",
        "profile = pipeline.start(config)\n",
        "\n",
        "# Get camera parameters\n",
        "intr = profile.get_stream(rs.stream.depth).as_video_stream_profile().get_intrinsics()\n",
        "scale = config.resolve(rs.pipeline_wrapper(pipeline)).get_device().first_depth_sensor().get_depth_scale()\n",
        "\n",
        "print(\"focal length(x) in pixels = \", intr.fx)\n",
        "print(\"focal length(y) in pixels = \", intr.fy)\n",
        "print(\"image height = \", intr.height)\n",
        "print(\"image width = \", intr.width)\n",
        "print(\"ppx = \", intr.ppx)\n",
        "print(\"ppy = \", intr.ppy)\n",
        "\n",
        "# Create a camera alignment object (depth aligned to color)\n",
        "align = rs.align(rs.stream.color)\n",
        "max_depth = 2.0 / scale # Zeros out for any depth greater than 2.0 meters\n",
        "\n",
        "# Display and save images\n",
        "print(\"Press [SPACE] to save images (png) and depth data (npy).\")\n",
        "print(\"Press 'q' to stop.\")\n",
        "nsaved = 0\n",
        "try:\n",
        "    while True:\n",
        "        # Wait for a coherent pair of frames: depth and color\n",
        "        frames = pipeline.wait_for_frames()\n",
        "        aligned_frames = align.process(frames)\n",
        "        color_frame = aligned_frames.get_color_frame()\n",
        "        depth_frame = aligned_frames.get_depth_frame()\n",
        "        if not depth_frame or not color_frame:\n",
        "            continue\n",
        "\n",
        "        # Convert images to numpy arrays\n",
        "        bgr = np.asanyarray(color_frame.get_data())\n",
        "        depth = np.asanyarray(depth_frame.get_data())\n",
        "        depth[depth > max_depth] = 0 # Zeros out\n",
        "\n",
        "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
        "        depth_colormap = cv2.applyColorMap(cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U), \n",
        "                                           cv2.COLORMAP_JET)\n",
        "  \n",
        "        images = np.hstack((bgr, depth_colormap))\n",
        "        # Show images\n",
        "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
        "        cv2.imshow('RealSense', images)\n",
        "        \n",
        "        key = cv2.waitKey(33)\n",
        "        if key == ord(' '):\n",
        "            Z = depth * scale * 1e+3 # unit in mm\n",
        "            color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "            \n",
        "            # Save images\n",
        "            cv2.imwrite('color{:02d}pc.png'.format(nsaved), bgr)\n",
        "            cv2.imwrite('depth{:02d}pc.png'.format(nsaved), depth_colormap)\n",
        "            np.save('Z{:02d}pc.npy'.format(nsaved), Z)\n",
        "            \n",
        "            print(\"color image and depth data are saved ({:02d})\".format(nsaved))\n",
        "            nsaved += 1\n",
        "\n",
        "        elif key == ord('q'):\n",
        "            if nsaved == 0:\n",
        "                Z = depth * scale * 1e+3 # unit in mm\n",
        "                color = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
        "            cv2.destroyAllWindows()\n",
        "            break\n",
        "        \n",
        "finally:\n",
        "    # Stop streaming\n",
        "    pipeline.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVHQq7WM2P4Z"
      },
      "source": [
        "## 逆透視変換でポイントクラウドを作りましょう．\n",
        "カラー画像および点群を作成する深度データを可視化します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FmUdTtztRBW"
      },
      "source": [
        "from PIL import Image\n",
        "height, width, _ = color.shape\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(color)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(Z, cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVXnNdq2P4a"
      },
      "source": [
        "### 各画素にuとvの座標を設定します．座標系は[予習事項](https://github.com/tsakailab/cisexpkit/blob/master/Experiment/Document/preparation.pdf)の図1です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVFSGtoEtRBX"
      },
      "source": [
        "# 主点を設定します．\n",
        "cx, cy = intr.ppx, intr.ppy # width*0.5, height*0.5\n",
        "j_to_u = lambda j: -(j - cx)\n",
        "i_to_v = lambda i: -(i - cy)\n",
        "# 画像平面の座標を設定します．\n",
        "u, v = np.meshgrid(j_to_u(np.arange(width)), i_to_v(np.arange(height)))\n",
        "print(u, u.shape)\n",
        "print(v, v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iittYCFC2P4b"
      },
      "source": [
        "### Z, u, vが与えられたとき，X[mm]とY[mm]を計算する関数を作りましょう．[ヒント：予習事項の問2](https://github.com/tsakailab/cisexpkit/blob/master/Experiment/Document/preparation.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMnVo0cOtRBZ"
      },
      "source": [
        "focal_length = [intr.fx, intr.fy] # in pixels\n",
        "def Zuv_to_XY(Z, u, v, f=focal_length):\n",
        "    ### X = ________   # Z, u, v, f[0], f[1] から必要なものを使って計算する\n",
        "    ### Y = ________   # Z, u, v, f[0], f[1] から必要なものを使って計算する\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OWR-JBMtRBa",
        "scrolled": true
      },
      "source": [
        "# Z, u, v から X, Y を計算します．\n",
        "X, Y = Zuv_to_XY(Z, u, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o4_QRe_2P4e"
      },
      "source": [
        "## ★ポイントクラウドを可視化して観察しましょう．\n",
        "> マウスで視点や拡大・縮小をコントロールできます．描画領域右上にある機能も活用しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_nPc9jtRBb",
        "scrolled": false
      },
      "source": [
        "nd = np.count_nonzero(Z)\n",
        "n = 30000\n",
        "p = np.random.choice(nd, min(n,nd), replace=False)\n",
        "print(\"%d out of %d points are displayed.\" % (n, nd))\n",
        "\n",
        "import plotly.graph_objs  as go\n",
        "rgb = color[Z>0][p] # * 1.5 # brighter\n",
        "\n",
        "trace = go.Scatter3d(x=X[Z>0][p], y=Y[Z>0][p], z=Z[Z>0][p], mode='markers',\n",
        "                     marker=dict(size=2, \n",
        "                                color=['rgb({},{},{})'.format(r,g,b) for r,g,b in zip(rgb[:,0], rgb[:,1], rgb[:,2])],\n",
        "                                opacity=0.8))\n",
        "\n",
        "layout = go.Layout(margin=dict(l=0,r=0,b=0,t=0))\n",
        "fig = go.Figure(data=[trace], layout=layout)\n",
        "camera = dict(up=dict(x=0, y=0, z=1), center=dict(x=0, y=-0.4, z=0), eye=dict(x=0, y=0.8, z=-2))\n",
        "fig.update_layout(scene_camera=camera)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxzOwEt2P4f"
      },
      "source": [
        "## 多面体の一辺の長さを計測しましょう．\n",
        "1. 多面体から辺をひとつ選んでください．\n",
        "2. 選んだ辺の両端の3次元座標を記録してください．\n",
        "3. 記録した端点の座標から，その辺の長さを算出してください．\n",
        "\n",
        "> 可視化したポイントクラウドの点にマウスを合わせると，その点の3次元座標(X,Y,Z)が表示されます．\n",
        "\n",
        "> 多面体の辺から端点の座標を読み取りやすいように，マウス操作による視点の操作や，scatter_3dの使い方を工夫しましょう．\n",
        "\n",
        "Q16: 計測を反復し，平均やばらつきなどで統計的に評価すべきである．計測した辺の長さの[正確度(accuracy)と精度(precision)](https://en.wikipedia.org/wiki/Accuracy_and_precision)を実験的に示せ．ただし，定規で測った辺の長さを真値とする．\n",
        "\n",
        "Q17: 辺の長さの3次元計測では，画像の取得から長さの推定までの過程において，どのような原因による誤差が考えられるか．正確度または精度を損なう誤差の原因をそれぞれひとつ以上指摘し，改善方法を提案せよ．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1cIgOEU2P4f"
      },
      "source": [
        "## 法線ベクトルと，なす角を計測しましょう．\n",
        "1. 多面体から面をふたつ選んでください．\n",
        "2. 選んだ面にある点の3次元座標を記録してください．ひとつの面から異なる3点の座標を記録します．\n",
        "3. 記録した3点の座標から，その面の単位法線ベクトル（法線の向きを表す，大きさ1の3次元ベクトル）を算出してください．\n",
        "4. 選んだ2面の法線ベクトルのなす間を計測してください．\n",
        "\n",
        "Q18: 計測した法線ベクトルのなす角を$\\theta$とする．精密な多面体を仮定すると，$\\cos\\theta$の真値はいくらか．\n",
        "\n",
        "Q19: 計測を反復し，平均やばらつきなどで統計的に評価すべきである．計測した$\\cos\\theta$または$\\theta$の[正確度(accuracy)と精度(precision)](https://en.wikipedia.org/wiki/Accuracy_and_precision)を実験的に示せ．\n",
        "\n",
        "Q20: この3次元計測では，画像の取得からなす角の推定までの過程において，どのような原因による誤差が考えられるか．正確度または精度を損なう誤差の原因をそれぞれひとつ以上指摘し，改善方法を提案せよ．ただし，多面体は精密に作られているものとする．"
      ]
    }
  ]
}